# LLMÂ SyntheticÂ PLM

Generate realistic **Productâ€‘Lifecycleâ€‘Management (PLM) metadata** from small, messy manufacturing datasets using a lightweight local languageâ€‘model.

---

## ğŸš€ Why this repo?

Manufacturing companies hold years of Billsâ€‘ofâ€‘Material, PDFs, CAD drawings, and ERP exportsâ€”but these files are scattered, inconsistent, and rarely machineâ€‘readable.  Fineâ€‘tuned language models can hallucinate when trained on tiny, noisy samples.  **LLMÂ SyntheticÂ PLM** tackles both problems by

1. **Ingesting unstructured sources** (Excel, CSV, PDFs, legacy PLM dumps)
2. **Profiling**  with GPTâ€‘4o api calls to draft a JSON cleaning plan based on column stats
3. **Applying deterministic** pandas transforms (rename, cast, impute, validate) driven by that plan
4. **Exporting** clean, auditâ€‘ready JSONL for analytics, reporting, or downstream model work

---

## ğŸ—‚Â Repository Layout

```text
LLM_synthetic_plm/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/          # Unmodified source files (Excel, CSV, PDFs)
â”‚   â”œâ”€â”€ processed/    # Cleaned, normalised JSONL ready for training
â”‚   â””â”€â”€ prompts/      # Prompt templates or fewâ€‘shot examples
â”‚
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ data_loader.py   # Parsers & validators for tabular + PDF + PLM exports
â”‚   â”œâ”€â”€ tokenizer.py     # Builds or loads a domain tokenizer (optional)
â”‚   â”œâ”€â”€ model.py         # NanoGPTâ€‘style TinyGPT architecture
â”‚   â””â”€â”€ trainer.py       # Training / fineâ€‘tuning loop with Lightning
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ prepare_data.py  # CLI: Excel/PDF â†’ JSONL
â”‚   â”œâ”€â”€ train.py         # CLI: train or fineâ€‘tune TinyGPT
â”‚   â””â”€â”€ generate.py      # CLI: emit synthetic metadata â†’ JSON
â”‚
â”œâ”€â”€ notebooks/           # Rapid experiments & EDA
â”œâ”€â”€ tests/               # pytest based unit tests
â”œâ”€â”€ models/              # Saved checkpoints (HF or safetensors)
â”œâ”€â”€ outputs/             # Generated samples & evaluation reports
â”œâ”€â”€ config.yaml          # Centralised paths + hyperâ€‘parameters
â””â”€â”€ requirements.txt
```

---

## âœ¨ Key Features

| Capability                    | Details                                                                                                  |
| ----------------------------- | -------------------------------------------------------------------------------------------------------- |
| **Multiâ€‘format ingestion**    | Parse Excel (.xls/.xlsx), CSV, PDF tables, and generic PLM XML/JSON dumps via a unified `DataLoader` API |
| **Schemaâ€‘aware cleaning**     | Automatic header mapping, unit conversion, duplicate collapse, and referentialâ€‘integrity checks          |
| **TinyGPT backbone**          | 2â€‘8â€¯M parameter model; train on consumer GPU / CPU in minutes                                            |
| **Promptâ€‘oriented training**  | Supports instructionâ€‘tuning and fewâ€‘shot augmentation to reduce hallucinations                           |
| **Deterministic JSON output** | Generates records conforming to a strict JSON schema (see `schemas/plm.json`)                            |
| **Evaluation suite**          | Bleu/BERTScore, schemaâ€‘conformance, and realâ€‘world plausibility checks                                   |

---

## ğŸ”§ Quickstart

1. **Clone & install**

   ```bash
   git clone https://github.com/<you>/LLM_synthetic_plm.git
   cd LLM_synthetic_plm
   python -m venv .venv && source .venv/bin/activate
   pip install -r requirements.txt
   ```
2. **Prepare data**
   Place your messy Excel or PDF files in `data/raw/` and run:

   ```bash
   python scripts/prepare_data.py \
       --src data/raw/*.xlsx data/raw/*.pdf \
       --dst data/processed/plm_train.jsonl
   ```
3. **Train / fineâ€‘tune**

   ```bash
   python scripts/train.py \
       --config config.yaml \
       --data data/processed/plm_train.jsonl \
       --save_dir models/tinygpt_plm
   ```
4. **Generate synthetic metadata**

   ```bash
   python scripts/generate.py \
       --ckpt models/tinygpt_plm/best.ckpt \
       --n 1000 \
       --out outputs/synthetic_plm.jsonl
   ```

*Result:* `outputs/synthetic_plm.jsonl` contains valid, diverse PLM rows ready for analytics, RAG pipelines, or further model preâ€‘training.

---

## âš™ï¸ Configuration

All hyperâ€‘parameters live in `config.yaml`:

```yaml
model:
  n_layers: 6
  n_heads: 6
  d_model: 384
training:
  batch_size: 64
  max_tokens: 128
  lr: 3eâ€‘4
  epochs: 8
```

Override any value via CLI flags, e.g. `--training.epochs 12`.

---

## ğŸ§© Extending the Repo

* **Additional file types**: add a new parser class to `modules/data_loader.py` and register it.
* **Bigger models**: swap `model.py` for a Huggingâ€¯Face architecture; config stays intact.
* **Structured validation**: extend `schemas/plm.json` and update `tests/` for new fields.

---

## ğŸ›¡ï¸ License

ApacheÂ 2.0â€”free for commercial and research use. See `LICENSE`.

---

## ğŸ¤ Contributing

PRs & issues are welcome!  Please run `preâ€‘commit` and `pytest` before opening a pull request.

---

## ğŸŒ Links & Inspiration

* [NanoGPT](https://github.com/karpathy/nanogpt)
* [LangChainÂ PDFÂ Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf)
* [Huggingâ€¯Face Datasets](https://huggingface.co/docs/datasets)

---

*Happy synthetic manufacturing!*
